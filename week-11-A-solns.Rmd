---
title: "Week 11, Day 1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(PPBDS.data)
library(knitr)
library(gt)
library(rstanarm)
library(tidyverse)
library(tidymodels)

# We will be using the `shaming` tibble from PPBDS.data. Check out ?shaming for
# details. On Day 1, we will explore the data and review the basics of Bayesian
# modeling, as covered in chapters 7 -- 9. On Day 2, we will decide on a model
# to use. On Day 3, we will use that model to answer questions.

# The full shaming data is huge. We will learn more about how to work with such
# large data sets next semester in Gov 1005: Big Data. Join us! For now, let's
# sample 10,000 rows and work with that.

set.seed(1005)
week_11 <- shaming %>% 
  sample_n(10000)
```

## Scene 1

**Prompt:** Let's explore the data. You can never look at your data too much!

1) How many voters got which treatments and how many voted in the 2006 primary? 

2) Explore `birth_year`. Does it make sense? If we got handed a new data set for today, would `birth_year` mean the same thing? Might we want to transform it into something different so that our model would "work" with today's data?

3) There are a bunch of voting records. What do they mean? Are they all recorded in the same way? How are they connected to each other? Do we want to use them all?

4) Explore the `no_of_names` variable? How is it distributed? What does it mean? Can we use it in our modeling?

5) Check out `hh_size`. What does it mean? Is the distribution sensible? Might it be a good idea to create a new variable which is more likely to capture an effect of interest? For example, I bet that that there is a big difference between living by yourself and living with other people. I bet that there is much less difference between living with 3 versus 4 people.

6) Are the factor levels for treatment convenient? Try a simple regression and see! How can we change them?

Perform other exploratory data analysis.  What other variables are connected to voting? What other variables are suspect/concerning?

7) Create a new data set, `week_11_clean`, which makes whatever corrections/improvements you think are a good idea. We will use that data set for the next two Scenes.



**Answers:** In talking with students (and course staff), it is clear that we should devote some more time to model-building, before we add in the complexity of tidymodels. The purpose of today's class is to do some guided exploration.

1) count() gives you a simple listing of number of votes. What is the easiest way to see percentages?

```{r}
week_11 %>% 
  count(treatment, primary_06)
```

2) `age` would be an easier to understand variable than `birth_year`. We don't believe that there is anything important about being born in year X. What matters is how old you are when the election occurs. So, `age` is a better variable to use. (This is either obvious or subtle.) 

3) We have records of whether or not someone voted in a given election. The last two digits are the year, and "general" versus "primary" tells us the election type. Note:

  +`general_04` is always "Yes." This is because the entire experiment was conducted by, first, identifying everyone who voted in the 2004 general election. If you did not do that, you were not allowed in the experiment. So, this variable is useless.
  
  + primary_06 is 0/1. This is the outcome of interest. When creating this tibble, we know that 0/1 would be more convenient for coding an outcome variable.
  
  + The other election variables are Yes/No character, which is fine. We could have code them as 0/1 but, in general, it is easier to make the pair of *variable name* and *values for that variable* make sense together. So, if the value for person $i$ of `general_04` is "Yes", then it means that they did vote in the 2004 general election. 


4) `no_of_names` has lots of missing values. Missing values are a whole other course in themselves. So, the simple solution is to not use them in this analysis. Note, also, that the missingness has a pattern! 

```{r}
table(week_11$treatment, is.na(week_11$no_of_names))
```

As the help page explains, no_of_names is only recorded for one of our treatments. So, if we try to use it, weird errors will start to arise when we try to calculate a causal effect.

5) `hh_size` is interesting. Lots of people live alone. I bet those people are different than others. I bet the differences between people who live in households of 3 versus 4 is much less important. Make `solo` equal to TRUE if someone lives alone, FALSE otherwise.

6) It would be much more convenient if the factor levels for treatment were ordered so that Control was the first level. That will make the interpretation of the coefficients in Scene 2 much easier. 

7) Clean up code:

```{r}
week_11_clean <- week_11 %>% 
  mutate(age = 2006 - birth_year) %>% 
  mutate(treatment = fct_relevel(treatment, "Control")) %>% 
  mutate(solo = ifelse(hh_size == 1, TRUE, FALSE)) %>% 
  select(-general_04, -no_of_names, -birth_year, -hh_size)
```

## Scene 2

**Prompt:** Having cleaned up our data, we are now ready to start modeling. 

* Let's be disciplined. Split up the data and only use the training data for the rest of today. 

* Use stan_glm() to estimate a model of `primary_06` as a function of `treatment`. Write a sentence or two interpreting the important parameters. (Hint: Try it both with and without an intercept.)

* Use the value of MAD_SD to discuss the magnitude/importance of various coefficients. Refer to this image, courtesy of Tyler.

```{r, echo=FALSE}
knitr::include_graphics("simko_importance.png")
```


* What is the causal effect?

* What is the meaning --- in words and mathematically --- of something like `treatmentSelf`? After all, it is not a variable in our data set . . .

* Compare the model with the intercept to the one without. Are they the same? Explain.


**Answers:** 

* Usual data split up rules.


```{r}
set.seed(9)
week_11_split <- initial_split(week_11_clean)
week_11_train <- testing(week_11_split)
week_11_test  <- testing(week_11_split)
week_11_folds <- vfold_cv(week_11_train, v = 5)
```


```{r}
fit_1 <- stan_glm(primary_06 ~ treatment, 
                  data = week_11_train,
                  refresh = 0)

fit_1_no_i <- stan_glm(primary_06 ~ treatment - 1, 
                  data = week_11_train,
                  refresh = 0)
```


```{r}
print(fit_1, digits = 3)
```


```{r}
print(fit_1_no_i, digits = 3, detail = FALSE)
```


* We don't talk about testing much because a) tests are a bad idea and, b) tests are covered thoroughly in other statistics courses. Recall my rant: https://davidkane9.github.io/PPBDS/n-parameters.html#testing. But we still need to discuss importance.

* The first sense of importance has to do with the raw magnitude of a coefficient. Consider the regression with a constant term. In that regression, the coefficient of the Neighbors treatment is 0.048, meaning people who got that treatment were 4.8% more likely to vote than those who got the Civic Duty message (which is the intercept) and 6.4% more likely to vote than those who got the control. Is tha a big number or a small number? Only intelligent reflection and discussion of the subject matter will tell you. 

* The second sense of importance concerns how certain you are about the result. How likely is it to, in truth, actually be zero, or even have the opposite sign. This is where we, mentally, add +/- 2 times the MAD SD to get a rough and ready 95% confidence interval. In the case of the Neighbors coefficient, the MAD SD is 0.02. So, the 95% confidence interval does not include zero. That is a good signal of importance.

* We are purposely leaving aside all the machinery of "statistical significance" and "null hypothesis tests." Again, see my rant for my opinions. For the students, the critical skill is the ability to discern how "important" a given variable is, using the two notions of importance above.  Make sure to connect this discussion to Tyler's (excellent!) grahic.

* Never hurts to explain again the connection between regressions with and without a constant term. Mathematically, they are identical, as long as there is at least one otherc categorical variable. That variable will then take the place of the intercept.

* There are many possible causal effects that one might be interested in, not just one! Recall that a causal effect is the difference between two potential outcomes. We have 5 potential outcomes, which means we have 10 different causal effects. Which one is most important? Depends on who you are and what question you are trying to answer!

* Discuss the construction of treatmentSelf. Students sometimes find this confusing. They understand that treatment is a variable, which is in the formula. They understand, in the math, that one might have a 0/1 variable (like Republican and Democrat as few weeks ago) which might be 1 if you got the Self treatment and 0 otherwise. And that, of course, is what treatmentSelf is. But the process by which R (magically!) transforms treatment into 4 dummy variables is opaque at best.

* Are the models the same? No and yes! They are, obviously, not the same. The coefficient of treatmentSelf is, for example, different. But, in all (?) the ways that really matter, they are the same. For example, their predictions are identical.

```{r}
tibble(pred_1 = predict(fit_1),
       pred_2 = predict(fit_1_no_i)) %>% 
  ggplot(aes(pred_1, pred_2)) +
    geom_point()
```

I don't think we have the time to go into the math of this question. The critical issue is that, as long as you have at least one categorical predictor, it does not matter (except to ease of interpretation) whether or not you include an intercept.


## Scene 3

**Prompt:** Explore a variety models which explain `primary_06` as a function of the variables in our data set. Make sure to explore some interaction terms. 

* Come up with at least two models that a) you like and would be willing to defend and b) are somewhat different from one another. The two most common model types in these situations are "simple" and "full". The former includes a minimum number of variables. The latter errs on the side of variable inclusion and the creation of interaction terms.

* What does it mean if, for example, the coefficient of `treatmentNeighbors` varies across models? 

* Do things change if we start using all the data? Is there a danger in doing so?

**Answers:** 

Here is how I would do this in real life. (In fact, I would have done some of this during the initial data exploration. That is how I knew that `solo` would be a useful variable.)

First, include all the variables I am still considering:


```{r}
mod_3_all <- stan_glm(primary_06 ~ sex + age + primary_02 + 
                        general_02 + primary_04 + treatment  + 
                        solo, 
                  data = week_11_train,
                  refresh = 0)
```

```{r}
print(mod_3_all, detail = FALSE, digits = 3)
```

The voting variables all seem important. (I do worry that they might be excessively correlated with each other. With more time, I might try to pull out a common factor.) `sex` does not seem important, but there is little wrong with keeping around a variable, even if it does not seem that important. The more that my intuition (or my boss's intuition or the client's intuition or the reviewer's intuition . . .) says that something ought to matter, the more likely I am to keep it around. After all, *it changes the predicted outcome very little to do so.* `hh_size` does not seem important, and I want to make my model simpler anyway. 

Is that analysis the "truth?" No! But it is not unreasonable. And that is the best we can do.

Try a simpler model:

```{r}
mod_3_some <- stan_glm(primary_06 ~ age + primary_04 + treatment, 
                  data = week_11_train,
                  refresh = 0)
```

```{r}
print(mod_3_some, detail = FALSE, digits = 3)
```


Try a model with some interactions:


```{r}
mod_3_inter <- stan_glm(primary_06 ~ sex + age + primary_02 + solo + 
                        general_02 + primary_04 + treatment + treatment:solo, 
                  data = week_11_train,
                  refresh = 0)

print(mod_3_inter, detail = FALSE, digits = 3)
```


* Are these the only three models worth considering? No! There are no right answers! There is no *true* model! There are an infinite number of models which we might construct with these variables. There is no one best way to explore/test them all. 

* Note how sigma changes across models. The better the model, the closer the predictions are to the truth. The closer the predictions are to the truth, the lower the RMSE will be. Now, as we discussed last week, sigma is not the same thing as RMSE. But it is close enough that I will always look at sigma.

* But looking at sigma, when using all the data, as we are here is dangerous. Of course, I (and everyone I have ever met) does the same. But the danger is still there.

* So, just pick the model with the lowest sigma? No! This was the over-fitting lesson last week. Just because sigma is lower does not mean that you have a better model --- where "better" is defined as "will produce better predictions on new data." That danger is why the structure of tidymodels is so useful.

* Never hurts to spend some time on coefficient interpretation.

* What does coefficient variation imply? Tricky question. I think the book's discussion is useful: What is beta_0? from: https://davidkane9.github.io/PPBDS/n-parameters.html#justice-and-courage

* Things change if we start using all our data rather than just the 10,000 we have here. Note how, by chance, our current sample show Neighbors as the least effective treatment when, in truth (?), it is the most important. The danger, obviously, depends on how one interprets "all the data." If it is all the training data, than you are OK. If you start using all of `shaming`, then you might be overfitting in the way that you pick and choose variables to include.





